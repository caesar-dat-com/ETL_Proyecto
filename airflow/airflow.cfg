[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /usr/local/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
# This path must be absolute
dags_folder = /usr/local/airflow/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Logging level
logging_level = INFO

# Logging format
log_format = [%%(asctime)s] {%%(processName)s} %%(__class__.__name__)s (%%(filename)s:%%(lineno)d) %%(levelname)s - %%(message)s

# Logging format for filename
log_filename_template = {{ base_log_folder }}/{{ dag_id }}/{{ task_id }}/{{ execution_date }}/{{ try_number }}.log

# Logging format for processor logs
log_processor_filename_template = {{ base_log_folder }}/scheduler/{{ filename }}.log

# The class to use for running task instances in parallel.
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The file to which the unit test results will be written
unit_test_mode = False

[database]
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow

[logging]
# The class to use for performing task instance logging.
logging_config_class =

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /usr/local/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging.
remote_logging = False

# Logging level
logging_level = INFO

# Logging format
log_format = [%%(asctime)s] {%%(processName)s} %%(__class__.__name__)s (%%(filename)s:%%(lineno)d) %%(levelname)s - %%(message)s

# Logging format for filename
log_filename_template = {{ base_log_folder }}/{{ dag_id }}/{{ task_id }}/{{ execution_date }}/{{ try_number }}.log

# Logging format for processor logs
log_processor_filename_template = {{ base_log_folder }}/scheduler/{{ filename }}.log

[secrets]
backend = airflow.providers.hashicorp.secrets.vault.VaultBackend
backend_kwargs = {"path": "airflow", "url": "http://127.0.0.1:8200"}

[fernet]
fernet_key = 4-Ls7YvNOQJqN-PKccxF_y3_XJHOmAc5LCZSoFk2CBM=
