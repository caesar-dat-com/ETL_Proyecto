{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ AutomatizaciÃ³n y GestiÃ³n de Datos con Airflow: ðŸ“Š Un AnÃ¡lisis Exploratorio del Flujo de Trabajo de Archivos CSV y Kafka ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones de mÃ³dulos en Python ðŸ“š\n",
    "\n",
    "- `os`: MÃ³dulo que proporciona funciones para interactuar con el sistema operativo. Utilizado aquÃ­ para manejar rutas de archivos y directorios.\n",
    "- `shutil`: MÃ³dulo que ofrece operaciones de alto nivel en archivos y colecciones de archivos. Ayuda en operaciones como copiar y mover archivos.\n",
    "- `pandas`: Biblioteca de anÃ¡lisis de datos que proporciona estructuras de datos y herramientas de manipulaciÃ³n de datos de alto rendimiento y fÃ¡ciles de usar.\n",
    "- `datetime`: MÃ³dulo que permite manipular fechas y horas. Es crucial para definir fechas de inicio en las tareas programadas.\n",
    "- `airflow`: Framework para programar y coordinar la ejecuciÃ³n de tareas. Se importa el mÃ³dulo `DAG` para definir el objeto DAG y `PythonOperator` para ejecutar cÃ³digo Python como tareas en el DAG.\n",
    "- `kafka`: MÃ³dulo para interactuar con Apache Kafka, un sistema de mensajerÃ­a distribuido. Se importa `KafkaProducer` para enviar mensajes a un tÃ³pico de Kafka.\n",
    "- `json`: MÃ³dulo que permite la codificaciÃ³n y decodificaciÃ³n de datos en formato JSON, Ãºtil para el formateo de mensajes enviados a Kafka.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from kafka import KafkaProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `clear_directories` se utiliza para mantener los directorios limpios eliminando todos los archivos existentes dentro de una lista especÃ­fica de directorios. Esta operaciÃ³n es crucial para preparar el entorno de trabajo antes de ejecutar procesos que dependen de la ausencia de archivos residuales, asegurando asÃ­ un entorno limpio y organizado. ðŸ—‘ï¸âœ¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para limpiar directorios\n",
    "def clear_directories(directories):\n",
    "    for directory in directories:\n",
    "        dir_path = os.path.join(BASE_DIR, directory)\n",
    "        for filename in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `copy_csv_files` facilita la transferencia de archivos CSV que contienen una palabra clave especÃ­fica desde un directorio de origen a un destino. Primero, asegura que el directorio de destino exista, creÃ¡ndolo si es necesario. Luego, recorre todos los archivos en el directorio de origen, copiando aquellos que coinciden con la palabra clave y terminan en `.csv` al directorio de destino. Este proceso es fundamental para organizar y preparar datos para etapas posteriores de procesamiento. ðŸ”„ðŸ“\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para copiar archivos CSV basados en una palabra clave\n",
    "def copy_csv_files(source_dir, target_dir, keyword):\n",
    "    source_path = os.path.join(BASE_DIR, source_dir)\n",
    "    target_path = os.path.join(BASE_DIR, target_dir)\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "    for filename in os.listdir(source_path):\n",
    "        if keyword.lower() in filename.lower() and filename.endswith('.csv'):\n",
    "            shutil.copy(os.path.join(source_path, filename), os.path.join(target_path, filename))\n",
    "            print(f\"Copied: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `unify_csv_files` se encarga de consolidar mÃºltiples archivos CSV de un directorio fuente en un Ãºnico archivo CSV de destino. Este proceso comienza identificando y listando todos los archivos CSV en el directorio fuente. Posteriormente, lee cada uno de estos archivos y los combina en un solo DataFrame de Pandas, el cual es finalmente guardado en el archivo de destino especificado. Este mÃ©todo es esencial para simplificar la gestiÃ³n de datos al reducir mÃºltiples archivos a uno solo, facilitando anÃ¡lisis y procesamientos posteriores. ðŸ“ˆðŸ—‚ï¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para unificar archivos CSV\n",
    "def unify_csv_files(source_dir, target_file):\n",
    "    source_path = os.path.join(BASE_DIR, source_dir)\n",
    "    target_path = os.path.join(BASE_DIR, target_file)\n",
    "    all_files = [os.path.join(source_path, f) for f in os.listdir(source_path) if f.endswith('.csv')]\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in all_files])\n",
    "    combined_df.to_csv(target_path, index=False)\n",
    "    print(f\"Unified file saved to: {target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `remove_duplicates` estÃ¡ diseÃ±ada para depurar un archivo CSV especÃ­fico eliminando todas las filas duplicadas. Primero carga el archivo CSV desde una ruta especificada en un DataFrame de Pandas. Utiliza el mÃ©todo `drop_duplicates` de Pandas para eliminar cualquier duplicado presente, y luego guarda el DataFrame limpio de nuevo en el archivo original. Esta operaciÃ³n asegura la integridad y la unicidad de los datos, lo cual es crucial para anÃ¡lisis de datos precisos y fiables. ðŸš«ðŸ“„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para eliminar duplicados en un archivo CSV\n",
    "def remove_duplicates(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Removed duplicates from: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `remove_empty_rows` se encarga de limpiar un archivo CSV eliminando todas las filas que contengan valores vacÃ­os o nulos. Carga el archivo en un DataFrame de Pandas desde una ruta especÃ­fica, aplica el mÃ©todo `dropna` para eliminar todas las filas que no tengan datos completos, y guarda el DataFrame resultante de vuelta en el archivo, sin los Ã­ndices. Este proceso es vital para asegurar la calidad de los datos, eliminando entradas incompletas que podrÃ­an afectar negativamente el anÃ¡lisis posterior. ðŸ—‘ï¸ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para eliminar filas vacÃ­as en un archivo CSV\n",
    "def remove_empty_rows(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Removed empty rows from: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `sumar_duracion` estÃ¡ diseÃ±ada para calcular la suma total de valores en la columna \"DuraciÃ³n\" de un archivo CSV especificado. Inicia convirtiendo los valores de la columna a nÃºmeros, tratando los posibles formatos de nÃºmeros con comas como decimales. Luego, suma todos los valores vÃ¡lidos, excluyendo cualquier dato nulo o incorrecto, y devuelve la suma total. Si ocurre un error durante el proceso, captura la excepciÃ³n y devuelve `None`, proporcionando tambiÃ©n un mensaje de error detallado. Esta funciÃ³n es esencial para obtener mÃ©tricas agregadas rÃ¡pidamente, lo cual es Ãºtil en anÃ¡lisis y reportes de duraciones o tiempos. ðŸ“ŠðŸ”„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para calcular la sumatoria de la columna \"DuraciÃ³n\" en cada archivo CSV\n",
    "def sumar_duracion(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['DuraciÃ³n'] = pd.to_numeric(df['DuraciÃ³n'].str.replace(',', '.'), errors='coerce')\n",
    "        suma_duracion = df['DuraciÃ³n'].dropna().sum()\n",
    "        print(f\"Total Duration for {source_file}: {suma_duracion}\")\n",
    "        return suma_duracion\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `sumar_duracion` estÃ¡ diseÃ±ada para calcular la suma total de valores en la columna \"DuraciÃ³n\" de un archivo CSV especificado. Inicia convirtiendo los valores de la columna a nÃºmeros, tratando los posibles formatos de nÃºmeros con comas como decimales. Luego, suma todos los valores vÃ¡lidos, excluyendo cualquier dato nulo o incorrecto, y devuelve la suma total. Si ocurre un error durante el proceso, captura la excepciÃ³n y devuelve `None`, proporcionando tambiÃ©n un mensaje de error detallado. Esta funciÃ³n es esencial para obtener mÃ©tricas agregadas rÃ¡pidamente, lo cual es Ãºtil en anÃ¡lisis y reportes de duraciones o tiempos. ðŸ“ŠðŸ”„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n para enviar datos a Kafka\n",
    "def send_to_kafka(source_file, topic):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                             value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = pd.read_csv(file)\n",
    "        for index, row in data.iterrows():\n",
    "            message = row.to_dict()\n",
    "            producer.send(topic, value=message)\n",
    "            producer.flush()\n",
    "        print(f\"Data from {source_file} sent to Kafka topic '{topic}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funciÃ³n `sumar_duracion_planeada` calcula la suma total de la columna \"DuraciÃ³n\" en un archivo CSV, utilizando una lÃ³gica condicional para determinar cuÃ¡l columna adicional ('Nombre' o 'codigo_personal_text') debe ser considerada para filtrar los datos. Primero intenta identificar y utilizar la columna 'Nombre', y si no es aplicable, usa 'codigo_personal_text'. DespuÃ©s de seleccionar la columna adecuada, convierte los valores de \"DuraciÃ³n\" a nÃºmeros, filtrando por las entradas vÃ¡lidas en la columna seleccionada, y luego suma estos valores. Esta funciÃ³n es Ãºtil para analizar duraciones planificadas, proporcionando una suma especÃ­fica basada en criterios de filtrado adicionales, y maneja errores para asegurar la robustez del proceso. Si no se encuentra una columna relevante o si ocurre un error, la funciÃ³n devuelve `None` y reporta el problema. ðŸ§ðŸ“ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumar_duracion_planeada(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'Nombre' in df.columns and df['Nombre'].notna().any():\n",
    "            columna_usada = 'Nombre'\n",
    "        elif 'codigo_personal_text' in df.columns and df['codigo_personal_text'].notna().any():\n",
    "            columna_usada = 'codigo_personal_text'\n",
    "        else:\n",
    "            print(\"Columna relevante no encontrada\")\n",
    "            return None\n",
    "        df['DuraciÃ³n'] = pd.to_numeric(df['DuraciÃ³n'].str.replace(',', '.'), errors='coerce')\n",
    "        df_filtrado = df[df[columna_usada].notna() & df[columna_usada].str.strip().astype(bool)]\n",
    "        suma_duracion_planeada = df_filtrado['DuraciÃ³n'].dropna().sum()\n",
    "        print(f\"Suma de duraciÃ³n planeada para {source_file} usando columna {columna_usada}: {suma_duracion_planeada}\")\n",
    "        return suma_duracion_planeada\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diccionario `default_args` define los argumentos predeterminados para las tareas en un DAG de Airflow. Estos incluyen:\n",
    "\n",
    "- `owner`: Define el propietario del DAG, en este caso, 'airflow'.\n",
    "- `depends_on_past`: Indica si la ejecuciÃ³n de la tarea depende de la finalizaciÃ³n exitosa de la misma tarea en la ejecuciÃ³n anterior. AquÃ­ estÃ¡ configurado como `False`, lo que permite la independencia entre ejecuciones.\n",
    "- `start_date`: Especifica la fecha de inicio de la primera ejecuciÃ³n del DAG; aquÃ­ se configura para el 24 de mayo de 2024.\n",
    "- `email_on_failure`: Si se debe enviar un correo electrÃ³nico en caso de fallo de la tarea. EstÃ¡ configurado como `False`, por lo que no se enviarÃ¡n correos automÃ¡ticamente.\n",
    "- `email_on_retry`: Similar a `email_on_failure`, determina si se envÃ­a un correo al reintentar la tarea, tambiÃ©n configurado como `False` aquÃ­.\n",
    "- `retries`: Define el nÃºmero mÃ¡ximo de reintentos en caso de fallo de una tarea. EstÃ¡ establecido en `1`, permitiendo un reintento.\n",
    "\n",
    "Estos argumentos son cruciales para el manejo de errores y la automatizaciÃ³n de notificaciones dentro de los flujos de trabajo de Airflow, proporcionando un control robusto y configurable sobre la ejecuciÃ³n de las tareas. ðŸš€ðŸ”§\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 5, 24),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConfiguraciÃ³n del DAG `indicadores_dag` en Airflow\n",
    "\n",
    "Este DAG, llamado `indicadores_dag`, estÃ¡ diseÃ±ado para realizar una serie de tareas relacionadas con el manejo y procesamiento de archivos CSV diariamente:\n",
    "\n",
    "- **DescripciÃ³n del DAG**: El DAG se encarga de limpiar directorios, copiar archivos, unificar datos, eliminar duplicados y filas vacÃ­as, calcular duraciones totales, y enviar datos a Kafka.\n",
    "- **Intervalo de ProgramaciÃ³n**: Se ejecuta diariamente (`@daily`).\n",
    "- **No RecuperaciÃ³n**: Configurado con `catchup=False` para evitar la ejecuciÃ³n de fechas pasadas que no se hayan ejecutado.\n",
    "\n",
    "### Tareas Definidas en el DAG\n",
    "\n",
    "1. **Limpiar Directorios**: Elimina todos los archivos dentro de los directorios especificados para asegurar un entorno limpio antes de procesar nuevos datos.\n",
    "2. **Copiar Archivos de Actividades e Inspecciones**: Copia archivos CSV que contienen ciertas palabras clave desde un directorio de origen a un directorio de destino.\n",
    "3. **Unificar Archivos de Actividades e Inspecciones**: Combina varios archivos CSV en un Ãºnico archivo, facilitando el procesamiento posterior.\n",
    "4. **Eliminar Duplicados en Archivos Unificados**: Asegura que los archivos combinados no contengan registros duplicados, manteniendo la calidad de los datos.\n",
    "5. **Eliminar Filas VacÃ­as en Archivos Unificados**: Remueve filas que no contengan informaciÃ³n, purificando aÃºn mÃ¡s los datos.\n",
    "6. **Calcular DuraciÃ³n Total y Planeada**: Suma los valores en la columna \"DuraciÃ³n\" de los archivos unificados, proporcionando mÃ©tricas claves para anÃ¡lisis.\n",
    "7. **Enviar Datos a Kafka**: Los datos limpios y procesados se envÃ­an a tÃ³picos especÃ­ficos en Kafka para su uso en sistemas downstream o anÃ¡lisis en tiempo real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(\n",
    "    'indicadores_dag',\n",
    "    default_args=default_args,\n",
    "    description='DAG that clears directories, copies, unifies, removes duplicates and empty rows, calculates total duration, and sends data to Kafka',\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    clear_directories_task = PythonOperator(\n",
    "        task_id='clear_directories',\n",
    "        python_callable=clear_directories,\n",
    "        op_kwargs={'directories': ['CSV_ACTIVIDAD', 'CSV_INSPECCION', 'CSV_unifi']},\n",
    "    )\n",
    "\n",
    "    copy_activities_task = PythonOperator(\n",
    "        task_id='copy_activities_csv_files',\n",
    "        python_callable=copy_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_DATA', 'target_dir': 'CSV_ACTIVIDAD', 'keyword': 'actividades'},\n",
    "    )\n",
    "\n",
    "    copy_inspecciones_task = PythonOperator(\n",
    "        task_id='copy_inspecciones_csv_files',\n",
    "        python_callable=copy_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_DATA', 'target_dir': 'CSV_INSPECCION', 'keyword': 'inspecciones'},\n",
    "    )\n",
    "\n",
    "    unify_activities_task = PythonOperator(\n",
    "        task_id='unify_activities_csv_files',\n",
    "        python_callable=unify_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_ACTIVIDAD', 'target_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    unify_inspecciones_task = PythonOperator(\n",
    "        task_id='unify_inspecciones_csv_files',\n",
    "        python_callable=unify_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_INSPECCION', 'target_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    remove_duplicates_activities_task = PythonOperator(\n",
    "        task_id='remove_duplicates_activities_csv_files',\n",
    "        python_callable=remove_duplicates,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    remove_duplicates_inspecciones_task = PythonOperator(\n",
    "        task_id='remove_duplicates_inspecciones_csv_files',\n",
    "        python_callable=remove_duplicates,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    remove_empty_rows_activities_task = PythonOperator(\n",
    "        task_id='remove_empty_rows_activities_csv_files',\n",
    "        python_callable=remove_empty_rows,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    remove_empty_rows_inspecciones_task = PythonOperator(\n",
    "        task_id='remove_empty_rows_inspecciones_csv_files',\n",
    "        python_callable=remove_empty_rows,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_activities_task = PythonOperator(\n",
    "        task_id='calculate_duration_activities',\n",
    "        python_callable=sumar_duracion,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_inspecciones_task = PythonOperator(\n",
    "        task_id='calculate_duration_inspecciones',\n",
    "        python_callable=sumar_duracion,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_planeada_activities_task = PythonOperator(\n",
    "    task_id='calculate_duration_planeada_activities',\n",
    "    python_callable=sumar_duracion_planeada,\n",
    "    op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_planeada_inspecciones_task = PythonOperator(\n",
    "    task_id='calculate_duration_planeada_inspecciones',\n",
    "    python_callable=sumar_duracion_planeada,\n",
    "    op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    send_to_kafka_activities_task = PythonOperator(\n",
    "        task_id='send_to_kafka_activities',\n",
    "        python_callable=send_to_kafka,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv', 'topic': 'activities_topic'},\n",
    "    )\n",
    "\n",
    "    send_to_kafka_inspecciones_task = PythonOperator(\n",
    "        task_id='send_to_kafka_inspecciones',\n",
    "        python_callable=send_to_kafka,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv', 'topic': 'inspecciones_topic'},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujo de Tareas en el DAG `indicadores_dag`\n",
    "\n",
    "El flujo de tareas en el DAG `indicadores_dag` estÃ¡ diseÃ±ado para asegurar que las operaciones se ejecuten en un orden lÃ³gico y eficiente. A continuaciÃ³n se describe cÃ³mo se encadenan las tareas:\n",
    "\n",
    "1. **InicializaciÃ³n de la Limpieza de Directorios**:\n",
    "   - La tarea `clear_directories_task` se ejecuta primero, asegurando que los directorios especificados estÃ©n limpios antes de comenzar cualquier otra operaciÃ³n.\n",
    "   \n",
    "2. **Copia de Archivos CSV**:\n",
    "   - DespuÃ©s de la limpieza, dos tareas se ejecutan en paralelo:\n",
    "     - `copy_activities_task`: Copia archivos relacionados con actividades.\n",
    "     - `copy_inspecciones_task`: Copia archivos relacionados con inspecciones.\n",
    "\n",
    "3. **UnificaciÃ³n de Archivos CSV**:\n",
    "   - `copy_activities_task` estÃ¡ seguido por:\n",
    "     - `unify_activities_task`: Unifica los archivos de actividades.\n",
    "   - `copy_inspecciones_task` estÃ¡ seguido por:\n",
    "     - `unify_inspecciones_task`: Unifica los archivos de inspecciones.\n",
    "\n",
    "4. **EliminaciÃ³n de Duplicados y Filas VacÃ­as**:\n",
    "   - DespuÃ©s de unificar los archivos, las tareas de eliminaciÃ³n de duplicados y filas vacÃ­as se ejecutan secuencialmente:\n",
    "     - `unify_activities_task` â†’ `remove_duplicates_activities_task` â†’ `remove_empty_rows_activities_task`\n",
    "     - `unify_inspecciones_task` â†’ `remove_duplicates_inspecciones_task` â†’ `remove_empty_rows_inspecciones_task`\n",
    "\n",
    "5. **CÃ¡lculo de DuraciÃ³n Total y Planeada**:\n",
    "   - Una vez limpios, los archivos son procesados para calcular la duraciÃ³n:\n",
    "     - `remove_empty_rows_activities_task` â†’ `calculate_duration_activities_task` â†’ `calculate_duration_planeada_activities_task`\n",
    "     - `remove_empty_rows_inspecciones_task` â†’ `calculate_duration_inspecciones_task` â†’ `calculate_duration_planeada_inspecciones_task`\n",
    "\n",
    "6. **EnvÃ­o de Datos a Kafka**:\n",
    "   - Finalmente, los datos procesados se envÃ­an a Kafka:\n",
    "     - `calculate_duration_planeada_activities_task` â†’ `send_to_kafka_activities_task`\n",
    "     - `calculate_duration_planeada_inspecciones_task` â†’ `send_to_kafka_inspecciones_task`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clear_directories_task >> [copy_activities_task, copy_inspecciones_task]\n",
    "    copy_activities_task >> unify_activities_task >> remove_duplicates_activities_task >> remove_empty_rows_activities_task >> calculate_duration_activities_task >> calculate_duration_planeada_activities_task >> send_to_kafka_activities_task\n",
    "    copy_inspecciones_task >> unify_inspecciones_task >> remove_duplicates_inspecciones_task >> remove_empty_rows_inspecciones_task >> calculate_duration_inspecciones_task >> calculate_duration_planeada_inspecciones_task >> send_to_kafka_inspecciones_task\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
