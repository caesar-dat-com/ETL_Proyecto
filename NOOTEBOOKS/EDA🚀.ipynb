{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 Automatización y Gestión de Datos con Airflow: 📊 Un Análisis Exploratorio del Flujo de Trabajo de Archivos CSV y Kafka 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones de módulos en Python 📚\n",
    "\n",
    "- `os`: Módulo que proporciona funciones para interactuar con el sistema operativo. Utilizado aquí para manejar rutas de archivos y directorios.\n",
    "- `shutil`: Módulo que ofrece operaciones de alto nivel en archivos y colecciones de archivos. Ayuda en operaciones como copiar y mover archivos.\n",
    "- `pandas`: Biblioteca de análisis de datos que proporciona estructuras de datos y herramientas de manipulación de datos de alto rendimiento y fáciles de usar.\n",
    "- `datetime`: Módulo que permite manipular fechas y horas. Es crucial para definir fechas de inicio en las tareas programadas.\n",
    "- `airflow`: Framework para programar y coordinar la ejecución de tareas. Se importa el módulo `DAG` para definir el objeto DAG y `PythonOperator` para ejecutar código Python como tareas en el DAG.\n",
    "- `kafka`: Módulo para interactuar con Apache Kafka, un sistema de mensajería distribuido. Se importa `KafkaProducer` para enviar mensajes a un tópico de Kafka.\n",
    "- `json`: Módulo que permite la codificación y decodificación de datos en formato JSON, útil para el formateo de mensajes enviados a Kafka.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from kafka import KafkaProducer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `clear_directories` se utiliza para mantener los directorios limpios eliminando todos los archivos existentes dentro de una lista específica de directorios. Esta operación es crucial para preparar el entorno de trabajo antes de ejecutar procesos que dependen de la ausencia de archivos residuales, asegurando así un entorno limpio y organizado. 🗑️✨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar directorios\n",
    "def clear_directories(directories):\n",
    "    for directory in directories:\n",
    "        dir_path = os.path.join(BASE_DIR, directory)\n",
    "        for filename in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "                print(f\"Removed: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `copy_csv_files` facilita la transferencia de archivos CSV que contienen una palabra clave específica desde un directorio de origen a un destino. Primero, asegura que el directorio de destino exista, creándolo si es necesario. Luego, recorre todos los archivos en el directorio de origen, copiando aquellos que coinciden con la palabra clave y terminan en `.csv` al directorio de destino. Este proceso es fundamental para organizar y preparar datos para etapas posteriores de procesamiento. 🔄📁\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para copiar archivos CSV basados en una palabra clave\n",
    "def copy_csv_files(source_dir, target_dir, keyword):\n",
    "    source_path = os.path.join(BASE_DIR, source_dir)\n",
    "    target_path = os.path.join(BASE_DIR, target_dir)\n",
    "    os.makedirs(target_path, exist_ok=True)\n",
    "    for filename in os.listdir(source_path):\n",
    "        if keyword.lower() in filename.lower() and filename.endswith('.csv'):\n",
    "            shutil.copy(os.path.join(source_path, filename), os.path.join(target_path, filename))\n",
    "            print(f\"Copied: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `unify_csv_files` se encarga de consolidar múltiples archivos CSV de un directorio fuente en un único archivo CSV de destino. Este proceso comienza identificando y listando todos los archivos CSV en el directorio fuente. Posteriormente, lee cada uno de estos archivos y los combina en un solo DataFrame de Pandas, el cual es finalmente guardado en el archivo de destino especificado. Este método es esencial para simplificar la gestión de datos al reducir múltiples archivos a uno solo, facilitando análisis y procesamientos posteriores. 📈🗂️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para unificar archivos CSV\n",
    "def unify_csv_files(source_dir, target_file):\n",
    "    source_path = os.path.join(BASE_DIR, source_dir)\n",
    "    target_path = os.path.join(BASE_DIR, target_file)\n",
    "    all_files = [os.path.join(source_path, f) for f in os.listdir(source_path) if f.endswith('.csv')]\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in all_files])\n",
    "    combined_df.to_csv(target_path, index=False)\n",
    "    print(f\"Unified file saved to: {target_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `remove_duplicates` está diseñada para depurar un archivo CSV específico eliminando todas las filas duplicadas. Primero carga el archivo CSV desde una ruta especificada en un DataFrame de Pandas. Utiliza el método `drop_duplicates` de Pandas para eliminar cualquier duplicado presente, y luego guarda el DataFrame limpio de nuevo en el archivo original. Esta operación asegura la integridad y la unicidad de los datos, lo cual es crucial para análisis de datos precisos y fiables. 🚫📄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para eliminar duplicados en un archivo CSV\n",
    "def remove_duplicates(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Removed duplicates from: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `remove_empty_rows` se encarga de limpiar un archivo CSV eliminando todas las filas que contengan valores vacíos o nulos. Carga el archivo en un DataFrame de Pandas desde una ruta específica, aplica el método `dropna` para eliminar todas las filas que no tengan datos completos, y guarda el DataFrame resultante de vuelta en el archivo, sin los índices. Este proceso es vital para asegurar la calidad de los datos, eliminando entradas incompletas que podrían afectar negativamente el análisis posterior. 🗑️🚀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para eliminar filas vacías en un archivo CSV\n",
    "def remove_empty_rows(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Removed empty rows from: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `sumar_duracion` está diseñada para calcular la suma total de valores en la columna \"Duración\" de un archivo CSV especificado. Inicia convirtiendo los valores de la columna a números, tratando los posibles formatos de números con comas como decimales. Luego, suma todos los valores válidos, excluyendo cualquier dato nulo o incorrecto, y devuelve la suma total. Si ocurre un error durante el proceso, captura la excepción y devuelve `None`, proporcionando también un mensaje de error detallado. Esta función es esencial para obtener métricas agregadas rápidamente, lo cual es útil en análisis y reportes de duraciones o tiempos. 📊🔄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la sumatoria de la columna \"Duración\" en cada archivo CSV\n",
    "def sumar_duracion(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['Duración'] = pd.to_numeric(df['Duración'].str.replace(',', '.'), errors='coerce')\n",
    "        suma_duracion = df['Duración'].dropna().sum()\n",
    "        print(f\"Total Duration for {source_file}: {suma_duracion}\")\n",
    "        return suma_duracion\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `sumar_duracion` está diseñada para calcular la suma total de valores en la columna \"Duración\" de un archivo CSV especificado. Inicia convirtiendo los valores de la columna a números, tratando los posibles formatos de números con comas como decimales. Luego, suma todos los valores válidos, excluyendo cualquier dato nulo o incorrecto, y devuelve la suma total. Si ocurre un error durante el proceso, captura la excepción y devuelve `None`, proporcionando también un mensaje de error detallado. Esta función es esencial para obtener métricas agregadas rápidamente, lo cual es útil en análisis y reportes de duraciones o tiempos. 📊🔄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para enviar datos a Kafka\n",
    "def send_to_kafka(source_file, topic):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                             value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = pd.read_csv(file)\n",
    "        for index, row in data.iterrows():\n",
    "            message = row.to_dict()\n",
    "            producer.send(topic, value=message)\n",
    "            producer.flush()\n",
    "        print(f\"Data from {source_file} sent to Kafka topic '{topic}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `sumar_duracion_planeada` calcula la suma total de la columna \"Duración\" en un archivo CSV, utilizando una lógica condicional para determinar cuál columna adicional ('Nombre' o 'codigo_personal_text') debe ser considerada para filtrar los datos. Primero intenta identificar y utilizar la columna 'Nombre', y si no es aplicable, usa 'codigo_personal_text'. Después de seleccionar la columna adecuada, convierte los valores de \"Duración\" a números, filtrando por las entradas válidas en la columna seleccionada, y luego suma estos valores. Esta función es útil para analizar duraciones planificadas, proporcionando una suma específica basada en criterios de filtrado adicionales, y maneja errores para asegurar la robustez del proceso. Si no se encuentra una columna relevante o si ocurre un error, la función devuelve `None` y reporta el problema. 🧐📈\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumar_duracion_planeada(source_file):\n",
    "    file_path = os.path.join(BASE_DIR, source_file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'Nombre' in df.columns and df['Nombre'].notna().any():\n",
    "            columna_usada = 'Nombre'\n",
    "        elif 'codigo_personal_text' in df.columns and df['codigo_personal_text'].notna().any():\n",
    "            columna_usada = 'codigo_personal_text'\n",
    "        else:\n",
    "            print(\"Columna relevante no encontrada\")\n",
    "            return None\n",
    "        df['Duración'] = pd.to_numeric(df['Duración'].str.replace(',', '.'), errors='coerce')\n",
    "        df_filtrado = df[df[columna_usada].notna() & df[columna_usada].str.strip().astype(bool)]\n",
    "        suma_duracion_planeada = df_filtrado['Duración'].dropna().sum()\n",
    "        print(f\"Suma de duración planeada para {source_file} usando columna {columna_usada}: {suma_duracion_planeada}\")\n",
    "        return suma_duracion_planeada\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {source_file}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diccionario `default_args` define los argumentos predeterminados para las tareas en un DAG de Airflow. Estos incluyen:\n",
    "\n",
    "- `owner`: Define el propietario del DAG, en este caso, 'airflow'.\n",
    "- `depends_on_past`: Indica si la ejecución de la tarea depende de la finalización exitosa de la misma tarea en la ejecución anterior. Aquí está configurado como `False`, lo que permite la independencia entre ejecuciones.\n",
    "- `start_date`: Especifica la fecha de inicio de la primera ejecución del DAG; aquí se configura para el 24 de mayo de 2024.\n",
    "- `email_on_failure`: Si se debe enviar un correo electrónico en caso de fallo de la tarea. Está configurado como `False`, por lo que no se enviarán correos automáticamente.\n",
    "- `email_on_retry`: Similar a `email_on_failure`, determina si se envía un correo al reintentar la tarea, también configurado como `False` aquí.\n",
    "- `retries`: Define el número máximo de reintentos en caso de fallo de una tarea. Está establecido en `1`, permitiendo un reintento.\n",
    "\n",
    "Estos argumentos son cruciales para el manejo de errores y la automatización de notificaciones dentro de los flujos de trabajo de Airflow, proporcionando un control robusto y configurable sobre la ejecución de las tareas. 🚀🔧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 5, 24),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del DAG `indicadores_dag` en Airflow\n",
    "\n",
    "Este DAG, llamado `indicadores_dag`, está diseñado para realizar una serie de tareas relacionadas con el manejo y procesamiento de archivos CSV diariamente:\n",
    "\n",
    "- **Descripción del DAG**: El DAG se encarga de limpiar directorios, copiar archivos, unificar datos, eliminar duplicados y filas vacías, calcular duraciones totales, y enviar datos a Kafka.\n",
    "- **Intervalo de Programación**: Se ejecuta diariamente (`@daily`).\n",
    "- **No Recuperación**: Configurado con `catchup=False` para evitar la ejecución de fechas pasadas que no se hayan ejecutado.\n",
    "\n",
    "### Tareas Definidas en el DAG\n",
    "\n",
    "1. **Limpiar Directorios**: Elimina todos los archivos dentro de los directorios especificados para asegurar un entorno limpio antes de procesar nuevos datos.\n",
    "2. **Copiar Archivos de Actividades e Inspecciones**: Copia archivos CSV que contienen ciertas palabras clave desde un directorio de origen a un directorio de destino.\n",
    "3. **Unificar Archivos de Actividades e Inspecciones**: Combina varios archivos CSV en un único archivo, facilitando el procesamiento posterior.\n",
    "4. **Eliminar Duplicados en Archivos Unificados**: Asegura que los archivos combinados no contengan registros duplicados, manteniendo la calidad de los datos.\n",
    "5. **Eliminar Filas Vacías en Archivos Unificados**: Remueve filas que no contengan información, purificando aún más los datos.\n",
    "6. **Calcular Duración Total y Planeada**: Suma los valores en la columna \"Duración\" de los archivos unificados, proporcionando métricas claves para análisis.\n",
    "7. **Enviar Datos a Kafka**: Los datos limpios y procesados se envían a tópicos específicos en Kafka para su uso en sistemas downstream o análisis en tiempo real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(\n",
    "    'indicadores_dag',\n",
    "    default_args=default_args,\n",
    "    description='DAG that clears directories, copies, unifies, removes duplicates and empty rows, calculates total duration, and sends data to Kafka',\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    clear_directories_task = PythonOperator(\n",
    "        task_id='clear_directories',\n",
    "        python_callable=clear_directories,\n",
    "        op_kwargs={'directories': ['CSV_ACTIVIDAD', 'CSV_INSPECCION', 'CSV_unifi']},\n",
    "    )\n",
    "\n",
    "    copy_activities_task = PythonOperator(\n",
    "        task_id='copy_activities_csv_files',\n",
    "        python_callable=copy_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_DATA', 'target_dir': 'CSV_ACTIVIDAD', 'keyword': 'actividades'},\n",
    "    )\n",
    "\n",
    "    copy_inspecciones_task = PythonOperator(\n",
    "        task_id='copy_inspecciones_csv_files',\n",
    "        python_callable=copy_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_DATA', 'target_dir': 'CSV_INSPECCION', 'keyword': 'inspecciones'},\n",
    "    )\n",
    "\n",
    "    unify_activities_task = PythonOperator(\n",
    "        task_id='unify_activities_csv_files',\n",
    "        python_callable=unify_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_ACTIVIDAD', 'target_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    unify_inspecciones_task = PythonOperator(\n",
    "        task_id='unify_inspecciones_csv_files',\n",
    "        python_callable=unify_csv_files,\n",
    "        op_kwargs={'source_dir': 'CSV_INSPECCION', 'target_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    remove_duplicates_activities_task = PythonOperator(\n",
    "        task_id='remove_duplicates_activities_csv_files',\n",
    "        python_callable=remove_duplicates,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    remove_duplicates_inspecciones_task = PythonOperator(\n",
    "        task_id='remove_duplicates_inspecciones_csv_files',\n",
    "        python_callable=remove_duplicates,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    remove_empty_rows_activities_task = PythonOperator(\n",
    "        task_id='remove_empty_rows_activities_csv_files',\n",
    "        python_callable=remove_empty_rows,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    remove_empty_rows_inspecciones_task = PythonOperator(\n",
    "        task_id='remove_empty_rows_inspecciones_csv_files',\n",
    "        python_callable=remove_empty_rows,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_activities_task = PythonOperator(\n",
    "        task_id='calculate_duration_activities',\n",
    "        python_callable=sumar_duracion,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_inspecciones_task = PythonOperator(\n",
    "        task_id='calculate_duration_inspecciones',\n",
    "        python_callable=sumar_duracion,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_planeada_activities_task = PythonOperator(\n",
    "    task_id='calculate_duration_planeada_activities',\n",
    "    python_callable=sumar_duracion_planeada,\n",
    "    op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv'},\n",
    "    )\n",
    "\n",
    "    calculate_duration_planeada_inspecciones_task = PythonOperator(\n",
    "    task_id='calculate_duration_planeada_inspecciones',\n",
    "    python_callable=sumar_duracion_planeada,\n",
    "    op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv'},\n",
    "    )\n",
    "\n",
    "    send_to_kafka_activities_task = PythonOperator(\n",
    "        task_id='send_to_kafka_activities',\n",
    "        python_callable=send_to_kafka,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_activities.csv', 'topic': 'activities_topic'},\n",
    "    )\n",
    "\n",
    "    send_to_kafka_inspecciones_task = PythonOperator(\n",
    "        task_id='send_to_kafka_inspecciones',\n",
    "        python_callable=send_to_kafka,\n",
    "        op_kwargs={'source_file': 'CSV_unifi/unified_inspecciones.csv', 'topic': 'inspecciones_topic'},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flujo de Tareas en el DAG `indicadores_dag`\n",
    "\n",
    "El flujo de tareas en el DAG `indicadores_dag` está diseñado para asegurar que las operaciones se ejecuten en un orden lógico y eficiente. A continuación se describe cómo se encadenan las tareas:\n",
    "\n",
    "1. **Inicialización de la Limpieza de Directorios**:\n",
    "   - La tarea `clear_directories_task` se ejecuta primero, asegurando que los directorios especificados estén limpios antes de comenzar cualquier otra operación.\n",
    "   \n",
    "2. **Copia de Archivos CSV**:\n",
    "   - Después de la limpieza, dos tareas se ejecutan en paralelo:\n",
    "     - `copy_activities_task`: Copia archivos relacionados con actividades.\n",
    "     - `copy_inspecciones_task`: Copia archivos relacionados con inspecciones.\n",
    "\n",
    "3. **Unificación de Archivos CSV**:\n",
    "   - `copy_activities_task` está seguido por:\n",
    "     - `unify_activities_task`: Unifica los archivos de actividades.\n",
    "   - `copy_inspecciones_task` está seguido por:\n",
    "     - `unify_inspecciones_task`: Unifica los archivos de inspecciones.\n",
    "\n",
    "4. **Eliminación de Duplicados y Filas Vacías**:\n",
    "   - Después de unificar los archivos, las tareas de eliminación de duplicados y filas vacías se ejecutan secuencialmente:\n",
    "     - `unify_activities_task` → `remove_duplicates_activities_task` → `remove_empty_rows_activities_task`\n",
    "     - `unify_inspecciones_task` → `remove_duplicates_inspecciones_task` → `remove_empty_rows_inspecciones_task`\n",
    "\n",
    "5. **Cálculo de Duración Total y Planeada**:\n",
    "   - Una vez limpios, los archivos son procesados para calcular la duración:\n",
    "     - `remove_empty_rows_activities_task` → `calculate_duration_activities_task` → `calculate_duration_planeada_activities_task`\n",
    "     - `remove_empty_rows_inspecciones_task` → `calculate_duration_inspecciones_task` → `calculate_duration_planeada_inspecciones_task`\n",
    "\n",
    "6. **Envío de Datos a Kafka**:\n",
    "   - Finalmente, los datos procesados se envían a Kafka:\n",
    "     - `calculate_duration_planeada_activities_task` → `send_to_kafka_activities_task`\n",
    "     - `calculate_duration_planeada_inspecciones_task` → `send_to_kafka_inspecciones_task`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clear_directories_task >> [copy_activities_task, copy_inspecciones_task]\n",
    "    copy_activities_task >> unify_activities_task >> remove_duplicates_activities_task >> remove_empty_rows_activities_task >> calculate_duration_activities_task >> calculate_duration_planeada_activities_task >> send_to_kafka_activities_task\n",
    "    copy_inspecciones_task >> unify_inspecciones_task >> remove_duplicates_inspecciones_task >> remove_empty_rows_inspecciones_task >> calculate_duration_inspecciones_task >> calculate_duration_planeada_inspecciones_task >> send_to_kafka_inspecciones_task\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
